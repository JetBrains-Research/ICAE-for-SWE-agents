### Model Arguments
model_name_or_path: "Qwen/Qwen3-8B" # mistralai/Mistral-7B-Instruct-v0.2
model_type: "icae" # "llm"
lora_r: 128 # 512
lora_dropout: 0.05
train: True # Needed for ICAE decoder
lora_target_modules: ["q_proj", "v_proj"]
use_position_identifiers: True
do_compress: True         # this is for inference in reality
freeze_encoder: False
freeze_decoder: True


### Data Arguments
max_train_samples: null
max_eval_samples: null
max_out_length: 15       # 15 tokens is mostly enough for squad
# Pretrain Data Arguments
dataset_repo: "DKYoon/SlimPajama-6B"
token_num: 1000000000 # 1B tokens
min_len: 1024
train_output_file: "icae/data/datasets/train_data_1024_4_1B.pt"
eval_output_file: "icae/data/datasets/eval_data_1024_4_1B.pt"
long_text_cache: "icae/data/datasets/long_texts.json"
eval_size: 500


### Training Arguments -- This depends on the pretraining or finetuning
output_dir: "icae/data/checkpoints/pretrain/pretrain_1108_1024_4_1B_IgorsFull"
overwrite_output_dir: False
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 0.0001  #0.00005 is 5e-5
max_grad_norm: 2.0
warmup_steps: 300
max_steps: 25100
num_train_epochs: 3
logging_steps: 25
train: True
do_train: True
do_eval: True
eval_strategy: "steps"
eval_steps: 500
eval_ae_num_samples: 10
save_steps: 3000
bf16: True
fp16: False
optim: "adamw_torch"
model_max_length: 4096
fixed_mem_size: 256
mean_compression_rate: 4
min_tokens_for_lm: 64
leave_tokens_for_lm: 0
lm_ratio: 0.5
add_special_token_for_lm: False
restore_from: "/mnt/shared-fs/slinko/projects/LLaMA-Factory/saves/Qwen3-8B/full/smith_qwen3-8b-no-think-full-lr5e-6-warmup100-adamw-torch____ft_xml_0p9/checkpoint-1767/"
dataloader_num_workers: 0
dataloader_pin_memory: True
dataloader_drop_last: True
seed: 42
gradient_checkpointing: False


### Inference Arguments
task: "ae" # "ae" or "lm"
restore_from: "/mnt/shared-fs/slinko/projects/LLaMA-Factory/saves/Qwen3-8B/full/smith_qwen3-8b-no-think-full-lr5e-6-warmup100-adamw-torch____ft_xml_0p9/checkpoint-1767/"
num_samples: 100
use_cpu: False
use_position_identifiers: True