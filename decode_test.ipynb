{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd218928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to decode token IDs from example.txt using Qwen3 tokenizer.\n",
    "Handles special memory tokens (IDs larger than vocab size) by replacing them with '<MT>'.\n",
    "Handles label masking tokens (-100) by replacing them with '<-100>'.\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List\n",
    "\n",
    "def safe_decode_with_mem_tokens(tokenizer, token_ids: List[int]) -> str:\n",
    "    \"\"\"\n",
    "    Decodes token IDs, replacing tokens larger than vocab size with '<MT>' \n",
    "    for special 'MEM_TOKEN' tokens, and -100 with '<-100>' for label masking.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        token_ids: List of token IDs to decode\n",
    "        \n",
    "    Returns:\n",
    "        Decoded string with memory tokens replaced by '<MT>' and -100 replaced by '<-100>'\n",
    "    \"\"\"\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    \n",
    "    # Convert IDs to tokens, replacing out-of-vocab IDs with None\n",
    "    tokens = []\n",
    "    for token_id in token_ids:\n",
    "        if token_id == -100:\n",
    "            # Label masking token\n",
    "            tokens.append(\"<-100>\")\n",
    "        elif token_id <= vocab_size + 50:\n",
    "            token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "            tokens.append(token)\n",
    "        else:\n",
    "            # Token ID is larger than vocab size - this is a memory token\n",
    "            tokens.append(\"<MT>\")\n",
    "    \n",
    "    # Convert tokens to string, replacing special placeholders as needed\n",
    "    result = tokenizer.convert_tokens_to_string(tokens)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen3 tokenizer from HuggingFace\n",
    "print(\"Loading Qwen3 tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", trust_remote_code=True)\n",
    "\n",
    "# Read the example.txt file\n",
    "print(\"Reading example.txt...\")\n",
    "with open('example.txt', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Parse the content (assuming it's a dictionary-like string)\n",
    "print(\"Parsing token IDs...\")\n",
    "try:\n",
    "    # Try to evaluate as a Python literal\n",
    "    data = ast.literal_eval(\"{\" + content + \"}\")\n",
    "except:\n",
    "    # If that fails, try to parse manually\n",
    "    import re\n",
    "    data = {}\n",
    "    \n",
    "    # Extract input_ids\n",
    "    input_ids_match = re.search(r\"'input_ids':\\s*\\[([\\d,\\s]+)\\]\", content)\n",
    "    if input_ids_match:\n",
    "        data['input_ids'] = [int(x.strip()) for x in input_ids_match.group(1).split(',') if x.strip()]\n",
    "    \n",
    "    # Extract prompt_answer_ids\n",
    "    prompt_answer_ids_match = re.search(r\"'prompt_answer_ids':\\s*\\[([\\d,\\s]+)\\]\", content)\n",
    "    if prompt_answer_ids_match:\n",
    "        data['prompt_answer_ids'] = [int(x.strip()) for x in prompt_answer_ids_match.group(1).split(',') if x.strip()]\n",
    "    \n",
    "    # Extract labels\n",
    "    labels_match = re.search(r\"'labels':\\s*\\[([\\d,\\s-]+)\\]\", content)\n",
    "    if labels_match:\n",
    "        data['labels'] = [int(x.strip()) for x in labels_match.group(1).split(',') if x.strip()]\n",
    "\n",
    "print(f\"\\nVocabulary size: {tokenizer.vocab_size}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Decode each field\n",
    "for key, token_ids in data.items():\n",
    "    if isinstance(token_ids, list) and token_ids:\n",
    "        print(f\"\\n{key}:\")\n",
    "        print(f\"  Length: {len(token_ids)} tokens\")\n",
    "        \n",
    "        # Count memory tokens\n",
    "        mem_token_count = sum(1 for tid in token_ids if tid > tokenizer.vocab_size)\n",
    "        if mem_token_count > 0:\n",
    "            print(f\"  Memory tokens: {mem_token_count}\")\n",
    "        \n",
    "        # Decode\n",
    "        decoded = safe_decode_with_mem_tokens(tokenizer, token_ids)\n",
    "        print(f\"  Decoded text:\")\n",
    "        print(decoded)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109cf0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91b356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135911d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16922a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98eef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
